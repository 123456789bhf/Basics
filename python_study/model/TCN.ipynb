{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCN\n",
    "参考连接：https://blog.csdn.net/qq_34107425/article/details/105522916\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积输出长度计算\n",
    "\n",
    "卷积操作的输出长度取决于输入长度、卷积核大小、步幅（stride）、填充（padding）和扩张（dilation）等参数。下面是输出长度的计算公式和示例。\n",
    "\n",
    "## 1. 标准卷积（没有扩张）\n",
    "\n",
    "对于标准的 1D 卷积，输出长度的计算公式为：\n",
    "\n",
    "$$\n",
    "\\text{Output Length} = \\left\\lfloor \\frac{\\text{Input Length} + 2 \\times \\text{Padding} - \\text{Kernel Size}}{\\text{Stride}} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "- **Input Length**：输入数据的时间步长（或序列长度）。\n",
    "- **Kernel Size**：卷积核的大小。\n",
    "- **Padding**：填充量，添加到输入两端的零值。\n",
    "- **Stride**：步幅，卷积核每次滑动的步长。\n",
    "- **Output Length**：卷积操作后的输出长度。\n",
    "\n",
    "## 2. 扩张卷积（Dilated Convolution）\n",
    "\n",
    "对于扩张卷积，输出长度的计算公式为：\n",
    "\n",
    "$$\n",
    "\\text{Output Length} = \\left\\lfloor \\frac{\\text{Input Length} + 2 \\times \\text{Padding} - (\\text{Kernel Size} - 1) \\times \\text{Dilation} - 1}{\\text{Stride}} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "- **Dilation**：扩张率，卷积核中元素之间的间距。\n",
    "\n",
    "### 3. 示例计算\n",
    "\n",
    "#### 示例 1：标准卷积\n",
    "\n",
    "- **输入长度**：10\n",
    "- **卷积核大小**：3\n",
    "- **填充**：1\n",
    "- **步幅**：1\n",
    "\n",
    "输出长度计算：\n",
    "\n",
    "$$\n",
    "\\text{Output Length} = \\left\\lfloor \\frac{10 + 2 \\times 1 - 3}{1} + 1 \\right\\rfloor = \\left\\lfloor \\frac{10 + 2 - 3}{1} + 1 \\right\\rfloor = \\left\\lfloor 9 + 1 \\right\\rfloor = 10\n",
    "$$\n",
    "\n",
    "因此，输出长度为 **10**。\n",
    "\n",
    "#### 示例 2：扩张卷积\n",
    "\n",
    "- **输入长度**：10\n",
    "- **卷积核大小**：3\n",
    "- **填充**：1\n",
    "- **步幅**：1\n",
    "- **扩张**：2\n",
    "\n",
    "输出长度计算：\n",
    "\n",
    "$$\n",
    "\\text{Output Length} = \\left\\lfloor \\frac{10 + 2 \\times 1 - (3 - 1) \\times 2 - 1}{1} + 1 \\right\\rfloor = \\left\\lfloor \\frac{10 + 2 - 4 - 1}{1} + 1 \\right\\rfloor = \\left\\lfloor 7 + 1 \\right\\rfloor = 8\n",
    "$$\n",
    "\n",
    "因此，输出长度为 **8**。\n",
    "\n",
    "#### 示例 3：带步幅的卷积\n",
    "\n",
    "- **输入长度**：10\n",
    "- **卷积核大小**：3\n",
    "- **填充**：1\n",
    "- **步幅**：2\n",
    "\n",
    "输出长度计算：\n",
    "\n",
    "$$\n",
    "\\text{Output Length} = \\left\\lfloor \\frac{10 + 2 \\times 1 - 3}{2} + 1 \\right\\rfloor = \\left\\lfloor \\frac{10 + 2 - 3}{2} + 1 \\right\\rfloor = \\left\\lfloor \\frac{9}{2} + 1 \\right\\rfloor = \\left\\lfloor 4.5 + 1 \\right\\rfloor = 5\n",
    "$$\n",
    "\n",
    "因此，输出长度为 **5**。\n",
    "\n",
    "## 4. 输出长度公式总结\n",
    "\n",
    "一般化的输出长度公式是：\n",
    "\n",
    "$$\n",
    "\\text{Output Length} = \\left\\lfloor \\frac{\\text{Input Length} + 2 \\times \\text{Padding} - (\\text{Kernel Size} - 1) \\times \\text{Dilation} - 1}{\\text{Stride}} + 1 \\right\\rfloor\n",
    "$$\n",
    "\n",
    "- **Padding**、**Dilation** 和 **Stride** 会影响输出长度的变化。\n",
    "- 增加步幅会缩小输出长度，增加填充或扩张会使输出长度增加。\n",
    "\n",
    "## 5. `Chomp1d` 的作用\n",
    "\n",
    "- **`Chomp1d`** 模块用于修剪卷积后可能多出来的部分，确保输出长度与输入长度一致，特别是在使用扩张卷积时。\n",
    "- 它通过去除卷积操作中多余的时间步，确保卷积后数据的时间步长与输入数据一致。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个函数是用来修剪卷积之后的数据的尺寸，让其与输入数据尺寸相同。\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个就是TCN的基本模块，包含8个部分，两个（卷积+修剪+relu+dropout）\n",
    "# 里面提到的downsample就是下采样，其实就是实现残差链接的部分。不理解的可以无视这个\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCN的主网络\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 16, 102]             160\n",
      "            Conv1d-2              [-1, 16, 102]             160\n",
      "           Chomp1d-3              [-1, 16, 100]               0\n",
      "           Chomp1d-4              [-1, 16, 100]               0\n",
      "              ReLU-5              [-1, 16, 100]               0\n",
      "              ReLU-6              [-1, 16, 100]               0\n",
      "           Dropout-7              [-1, 16, 100]               0\n",
      "           Dropout-8              [-1, 16, 100]               0\n",
      "            Conv1d-9              [-1, 16, 102]             784\n",
      "           Conv1d-10              [-1, 16, 102]             784\n",
      "          Chomp1d-11              [-1, 16, 100]               0\n",
      "          Chomp1d-12              [-1, 16, 100]               0\n",
      "             ReLU-13              [-1, 16, 100]               0\n",
      "             ReLU-14              [-1, 16, 100]               0\n",
      "          Dropout-15              [-1, 16, 100]               0\n",
      "          Dropout-16              [-1, 16, 100]               0\n",
      "           Conv1d-17              [-1, 16, 100]              64\n",
      "             ReLU-18              [-1, 16, 100]               0\n",
      "    TemporalBlock-19              [-1, 16, 100]               0\n",
      "           Conv1d-20              [-1, 32, 104]           1,568\n",
      "           Conv1d-21              [-1, 32, 104]           1,568\n",
      "          Chomp1d-22              [-1, 32, 100]               0\n",
      "          Chomp1d-23              [-1, 32, 100]               0\n",
      "             ReLU-24              [-1, 32, 100]               0\n",
      "             ReLU-25              [-1, 32, 100]               0\n",
      "          Dropout-26              [-1, 32, 100]               0\n",
      "          Dropout-27              [-1, 32, 100]               0\n",
      "           Conv1d-28              [-1, 32, 104]           3,104\n",
      "           Conv1d-29              [-1, 32, 104]           3,104\n",
      "          Chomp1d-30              [-1, 32, 100]               0\n",
      "          Chomp1d-31              [-1, 32, 100]               0\n",
      "             ReLU-32              [-1, 32, 100]               0\n",
      "             ReLU-33              [-1, 32, 100]               0\n",
      "          Dropout-34              [-1, 32, 100]               0\n",
      "          Dropout-35              [-1, 32, 100]               0\n",
      "           Conv1d-36              [-1, 32, 100]             544\n",
      "             ReLU-37              [-1, 32, 100]               0\n",
      "    TemporalBlock-38              [-1, 32, 100]               0\n",
      "           Conv1d-39              [-1, 64, 108]           6,208\n",
      "           Conv1d-40              [-1, 64, 108]           6,208\n",
      "          Chomp1d-41              [-1, 64, 100]               0\n",
      "          Chomp1d-42              [-1, 64, 100]               0\n",
      "             ReLU-43              [-1, 64, 100]               0\n",
      "             ReLU-44              [-1, 64, 100]               0\n",
      "          Dropout-45              [-1, 64, 100]               0\n",
      "          Dropout-46              [-1, 64, 100]               0\n",
      "           Conv1d-47              [-1, 64, 108]          12,352\n",
      "           Conv1d-48              [-1, 64, 108]          12,352\n",
      "          Chomp1d-49              [-1, 64, 100]               0\n",
      "          Chomp1d-50              [-1, 64, 100]               0\n",
      "             ReLU-51              [-1, 64, 100]               0\n",
      "             ReLU-52              [-1, 64, 100]               0\n",
      "          Dropout-53              [-1, 64, 100]               0\n",
      "          Dropout-54              [-1, 64, 100]               0\n",
      "           Conv1d-55              [-1, 64, 100]           2,112\n",
      "             ReLU-56              [-1, 64, 100]               0\n",
      "    TemporalBlock-57              [-1, 64, 100]               0\n",
      "================================================================\n",
      "Total params: 51,072\n",
      "Trainable params: 51,072\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.64\n",
      "Params size (MB): 0.19\n",
      "Estimated Total Size (MB): 1.84\n",
      "----------------------------------------------------------------\n",
      "Output shape: torch.Size([4, 64, 100])\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "\n",
    "# 检查是否有可用的 GPU，如果有则使用 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 假设的输入数据\n",
    "batch_size = 4   # 批次大小\n",
    "num_inputs = 3   # 输入通道数\n",
    "sequence_length = 100  # 序列长度\n",
    "\n",
    "# 创建一个形状为 (batch_size, num_inputs, sequence_length) 的假数据\n",
    "x = torch.randn(batch_size, num_inputs, sequence_length).to(device)\n",
    "\n",
    "# 定义 TCN 模型\n",
    "num_channels = [16, 32, 64]  # 定义每一层的输出通道数\n",
    "kernel_size = 3              # 卷积核大小\n",
    "dropout = 0.2                # Dropout 比例\n",
    "\n",
    "# 实例化 TemporalConvNet 模型\n",
    "model = TemporalConvNet(num_inputs, num_channels, kernel_size, dropout).to(device)\n",
    "\n",
    "# 打印模型的每一层信息和输出形状\n",
    "summary(model, input_size=(num_inputs, sequence_length))\n",
    "\n",
    "# 前向传播\n",
    "output = model(x)\n",
    "\n",
    "# 打印输出的形状\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
