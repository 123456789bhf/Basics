{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6745fa89",
   "metadata": {},
   "source": [
    "# 深度学习计算\n",
    "\n",
    "## 层和块\n",
    "    \n",
    "神经网络模型（或者单个层）：输入、输出与参数。输入在网络层中向前传播，误差经过梯度计算向后传播（BP算法）。深度学习广泛应用于cv、nlp等方向，和强化学习结合形成深度强化学习，在金融量化中也有着广泛的应用。在现在的深度学习任务中，神经网络的模型往往是很复杂的，层数会达到数百层。因此，研究网络的部分（组件）比研究单个层或者整个模型本身就显得更加重要。神经网络块相当于编程的一个类，它的任一子类都需要定义将输入转换为输出的向前传播函数，并且能存储必要的参数（块也可以没有参数）。由于有自动微分，反向传播函数与梯度计算由系统自动完成。神经网络块具有多功能性，可以描述单个的层、由多个层构成的组件，甚至整个模型本身。使用块的好处就是可以将块递归地拼接成一个更大的组件。\n",
    "\n",
    "RK:我们之前所说的层相当于网络模型的组成层，由多个神经元构成。比如隐藏层。现在说的层一般指模型中的操作层，比如线性层。当然，可以用线性层定义一个隐藏层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd8ae8",
   "metadata": {},
   "source": [
    "- torch.nn: pytorch自带的一个函数库，包含了神经网络中使用的一些常用的模块，比如nn.Linear、nn.Conv2d等。\n",
    "\n",
    "- nn.Module: 所有神经网络模块的基类，所有定义的模块都要继承它，常用于高级的功能模块、网络层的搭建。上述层的定义也继承了该基类。\n",
    "\n",
    "- nn.functional: 也是nn中常用的一个模块，nn中大多数层在functional中都有一个函数与之对应，比如F.linear、F.conv2d等。nn.Module和 \n",
    "    nn.functional在性能上没有太大差异，也能实现相同的功能。区别在于:nn中的层由类定义，继承nn.Module，可以自动提取可学习的参数；而   \n",
    "    nn.functional中的由函数定义。因此模块如果有可学习的参数，一般用nn.Module，并且放在构造函数__init__()中，比如线性层、卷积层；如 \n",
    "    果模型不具有可学习参数，则可以放在__init__()中也可以不放，当放在前向传播函数中时就用相应的函数代替，比如激活层、池化层。\n",
    "    \n",
    "- nn.Sequential: 继承自nn.Module，相当于一个可以包装层的有序容器，模块将按照构造函数中的传递顺序依次被添加到这个容器中执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3feee5b",
   "metadata": {},
   "source": [
    "下段代码通过实例化nn.Sequential类构建了一个全连接的神经网络，隐藏层与输出层都是nn.Linear类的实例化，并且层作为Sequential的参数，执行顺序是按照其参数传递的顺序进行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16d2640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "net=nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "X=torch.rand(2,20)\n",
    "net(X)\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e8b79",
   "metadata": {},
   "source": [
    "### 5.1.1 自定义块\n",
    "\n",
    "我们在自定义块需要知道：\n",
    "    \n",
    "- 需要继承nn.Module类（利用super()），并定义向前传播函数；\n",
    "\n",
    "- 层是否要放在构造函数中；\n",
    "    \n",
    "- 只要定义了前向传播函数，通过自动微分，系统自动实现反向传播函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cf5429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0023, -0.1800, -0.2153, -0.1431,  0.1024,  0.1512,  0.1314,  0.0312,\n",
       "          0.3083, -0.0305],\n",
       "        [ 0.0042, -0.1614, -0.0606, -0.0706,  0.1020,  0.2080,  0.0892, -0.2596,\n",
       "          0.2909, -0.0515]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        #调用父类的构造函数进行必要的初始化\n",
    "        super().__init__()\n",
    "        #定义两个全连接层，都是nn.Linear的实例化，并被赋予默认值\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))\n",
    "    \n",
    "net=MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96392ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd6bf16",
   "metadata": {},
   "source": [
    "F.relu()与nn.ReLU()的区别：F.relu()是函数调用，一般用在forward函数中；nn.ReLU()是模块调用，一般用于定义网络的层。\n",
    "块的基本功能有：\n",
    "\n",
    "- 将输入数据作为其向前传播函数的参数；\n",
    "\n",
    "- 通过向前传播函数来生成输出；\n",
    "\n",
    "- 计算输出关于输入的梯度，可通过其反向传播函数进行访问；\n",
    "\n",
    "- 存储和访问向前传播计算所需的参数；\n",
    "\n",
    "- 根据需要初始化参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e2c90",
   "metadata": {},
   "source": [
    "### 5.1.2 顺序块\n",
    "\n",
    "为了构建一个顺序块，需要定义两个关键函数：\n",
    "    \n",
    "- 一种将块逐个加入到列表中的函数。\n",
    "\n",
    "- 一种前向传播函数，用于将输入按照追加块的顺序进行传递。\n",
    "\n",
    "下段代码构建了一个顺序块MySequential，功能与Sequential类相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b3e6f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first animal's name is cat\n",
      "dog pig\n"
     ]
    }
   ],
   "source": [
    "#当不知道传递多少个实参的时候用*\n",
    "def animal(first_ani,*args):    \n",
    "    print(\"The first animal's name is\",first_ani)\n",
    "    print(*args)\n",
    "        \n",
    "animal('cat','dog','pig')#调用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a54550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x1037ab040>\n",
      "(0, 1)\n",
      "(1, 2)\n",
      "(2, 3)\n",
      "(3, 4)\n"
     ]
    }
   ],
   "source": [
    "#enumerate的用法\n",
    "s=[1,2,3,4]\n",
    "#此处遍历对象是列表，也可以是字符串、字典等\n",
    "e=enumerate(s)\n",
    "#返回的是一个enumerate对象，包含索引与值\n",
    "print(e)\n",
    "for index,value in e:\n",
    "    print((index,value))#访问索引以及其对应的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9476655c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1812, -0.1662, -0.1065,  0.1727, -0.2045,  0.1291,  0.0788,  0.1017,\n",
       "          0.3599, -0.0027],\n",
       "        [ 0.1275, -0.0719, -0.0779,  0.1029, -0.1994,  0.0236,  0.0592,  0.0708,\n",
       "          0.3223,  0.0426]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    #因为不知道传递多少数据那么用*\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for idx,module in enumerate(args):\n",
    "            self._modules[str(idx)]=module#有序字典\n",
    "            #idx被转换成字符串作为关键字，与module值构成键值对，依次存放于有序字典self_modules中。\n",
    "    #self._modules是有序字典\n",
    "    def forward(self,X):\n",
    "        #有序字典保证了按照存放顺序进行遍历\n",
    "        #访问有序字典的值，有序字典访问for index,values in 有序字典,如果只是访问值就用.values\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "        return X\n",
    "\n",
    "net=MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662aaeb0",
   "metadata": {},
   "source": [
    "### 在前向传播函数中执行代码\n",
    "\n",
    "Sequential类将模块简单地串联起来，使得模型构造变得简单。当需要更强的灵活性时，如想要执行python控制流、执行任意的数学运算等，已有的层不能完成这些任务，我们就还需要定义其他的块。\n",
    "\n",
    "有时候，我们会想让一些参数的值保持不变，而不是想去更新它，我们将这类参数称之为常数参数。下段代码实现了一个神经网络模型，不过与之前不同的是，我们添加了一个计算函数f(x)=w·x的层，并在前向传播函数中添加了一个控制流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b961f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1087, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)#不更新参数，从而是参数保持不变\n",
    "        #self.rand_weight就是常数参数，保持不变\n",
    "        self.linear=nn.Linear(20,20)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        X=self.linear(X)\n",
    "        X=F.relu(torch.mm(X,self.rand_weight))\n",
    "        #复用全连接层，参数共享\n",
    "        X=self.linear(X)\n",
    "        #控制流，只是给了告诉我们在定义模块的时候可以添加控制流\n",
    "        while X.abs().sum()>1:\n",
    "            X/=2\n",
    "        return X.sum()\n",
    "    \n",
    "net=FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7378266",
   "metadata": {},
   "source": [
    "我们可以混合搭配各种组合块的方法。下段代码实现了块的嵌套。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ec5779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1511, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                               nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera=nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5723a48",
   "metadata": {},
   "source": [
    "## 5.2 参数管理\n",
    "\n",
    "在选择了网络模型架构并设置必要的超参数后，假设空间就确定了。此时根据损失函数最小化的策略对网络进行训练，最终得到相应的参数，网络模型也就确定下来，然后对未知数据进行预测。当然，我们就需要将训练好的参数保存下来，以便在其他环境中复用它们。\n",
    "\n",
    "在前几节，我们只依靠深度学习框架来完成训练工作，对参数没有进行其他具体的操作。为此，接下来的内容，我们将介绍：\n",
    "\n",
    "- 访问参数，用于调试、诊断、可视化等；\n",
    "\n",
    "- 参数初始化；\n",
    "\n",
    "- 在不同模型组件间实现参数共享。\n",
    "\n",
    "我们从具有单隐藏层的多层感知机开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79d4175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4710],\n",
       "        [0.2716]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X=torch.rand(size=(2,4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf01231",
   "metadata": {},
   "source": [
    "### 5.2.1 参数访问\n",
    "\n",
    "首先，我们从已有的模型中访问参数。当实例化Sequential类定义模型时，我们可以将模型的层看作一个列表，通过索引的方式访问任何一个层，每层的参数都在其属性（可通过实例来访问的变量）中。我们可以来访问输出层的参数。其中，参数都存储为单精度浮点型，并且参数名称允许唯一标识每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d5085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2455, -0.0415,  0.3363, -0.1995,  0.2323, -0.2685,  0.0360,  0.3345]])), ('bias', tensor([0.1505]))])\n"
     ]
    }
   ],
   "source": [
    "print(net[2].state_dict())#.state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcebccc9",
   "metadata": {},
   "source": [
    "**目标参数**\n",
    "\n",
    "访问参数返回的是一个参数类，有参数值、梯度和额外信息。下段代码提取了输出层的偏置,并进一步访问了偏置的具体值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b35670ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.1505], requires_grad=True)\n",
      "tensor([0.1505])\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))  #返回偏置参数的类型\n",
    "print(net[2].bias)#可以更新参数\n",
    "print(net[2].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c04d758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad==None  #由于还没有调用反向传播，参数的梯度处于初始状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d8b5d",
   "metadata": {},
   "source": [
    "**一次性访问所有参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91648231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([256, 20])) ('bias', torch.Size([256]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#以元组形式输出每个层的参数，比较方便，如果单独输出用下面的方法就会比较麻烦\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39m[(name,param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnamed_parameters()])\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39m[(name,param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict()])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39m[(name,param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mnamed_parameters()])\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#以元组形式输出每个层的参数，比较方便，如果单独输出用下面的方法就会比较麻烦\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39m[(name,param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnamed_parameters()])\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39m[(name,param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict()])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m*\u001b[39m[(name,param\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mnamed_parameters()])\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "#以元组形式输出每个层的参数，比较方便，如果单独输出用下面的方法就会比较麻烦\n",
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "#print(*[(name,param.shape) for name,param in net[0].state_dict()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ff2da5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m ft\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#net[0].named_parameters()表示提取网络参数的名字和对应的值\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name,param \u001b[38;5;129;01min\u001b[39;00m net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstate_dict():\n\u001b[0;32m      4\u001b[0m     ft\u001b[38;5;241m.\u001b[39mappend((name,param\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(ft)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "ft=[]\n",
    "#net[0].named_parameters()表示提取网络参数的名字和对应的值\n",
    "for name,param in net[0].named_parameters():\n",
    "    ft.append((name,param.shape))\n",
    "print(ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b642207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "<class 'collections.OrderedDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.1690, -0.1930,  0.0068,  ..., -0.1027, -0.1727, -0.0273],\n",
       "                      [ 0.1415, -0.0509, -0.1632,  ...,  0.0715, -0.0977, -0.0544],\n",
       "                      [-0.0978,  0.0800, -0.0905,  ...,  0.0530,  0.0345,  0.1542],\n",
       "                      ...,\n",
       "                      [-0.0591,  0.1158,  0.1073,  ..., -0.1088,  0.0101,  0.0883],\n",
       "                      [ 0.2225,  0.1110,  0.0483,  ..., -0.1018,  0.2156, -0.1966],\n",
       "                      [ 0.0186,  0.0158,  0.1021,  ...,  0.1299,  0.2000, -0.1046]])),\n",
       "             ('bias',\n",
       "              tensor([ 0.0118,  0.0827, -0.1007,  0.1862, -0.1996,  0.0961,  0.1203,  0.1449,\n",
       "                      -0.0968, -0.0993, -0.1682, -0.2071,  0.1839,  0.1995, -0.1628,  0.1873,\n",
       "                      -0.0887, -0.0270,  0.0512,  0.1104,  0.0323, -0.1739, -0.2141, -0.2207,\n",
       "                       0.1328, -0.0419, -0.1421, -0.0786,  0.0513,  0.1232,  0.2155,  0.2114,\n",
       "                      -0.0615,  0.1347, -0.1348,  0.2211,  0.0923,  0.1982, -0.1227, -0.2196,\n",
       "                       0.1883, -0.0431, -0.0371, -0.1837,  0.0237, -0.0217,  0.2202,  0.1398,\n",
       "                       0.1685,  0.1856, -0.0293,  0.1566,  0.0490,  0.1776,  0.0366, -0.2087,\n",
       "                       0.2047, -0.0028,  0.1226, -0.0333,  0.1627,  0.0446,  0.0120,  0.0025,\n",
       "                       0.0468, -0.1092,  0.0566,  0.0638, -0.1634, -0.0042, -0.1577,  0.1248,\n",
       "                      -0.0256, -0.1310, -0.1030, -0.1515, -0.0205, -0.0436, -0.0249, -0.2112,\n",
       "                       0.1688,  0.1198,  0.0807, -0.1631, -0.0547,  0.0538, -0.1126, -0.1413,\n",
       "                      -0.0541,  0.0268, -0.0126, -0.0755, -0.1405, -0.0666, -0.0379,  0.2202,\n",
       "                       0.0856,  0.1354, -0.1670, -0.0639,  0.1014,  0.0881,  0.0249,  0.0197,\n",
       "                       0.1074,  0.1373,  0.1899,  0.0411, -0.1801, -0.1975,  0.1075, -0.1001,\n",
       "                      -0.1415, -0.2102,  0.1193,  0.1466, -0.1942,  0.2064, -0.0719, -0.1323,\n",
       "                       0.1895, -0.0230, -0.1925, -0.0498,  0.1173, -0.0039,  0.1590,  0.0433,\n",
       "                      -0.1105,  0.2153, -0.2095, -0.0842,  0.1593,  0.1899,  0.0929,  0.0731,\n",
       "                       0.1795,  0.0963,  0.1645,  0.1847,  0.2080, -0.1496,  0.0191,  0.0512,\n",
       "                       0.0714,  0.1629,  0.0971,  0.2011, -0.2079,  0.0049, -0.1472, -0.1074,\n",
       "                       0.1600, -0.1648,  0.2060,  0.0864,  0.1053, -0.0493,  0.0454,  0.1974,\n",
       "                      -0.0646, -0.0509, -0.1371,  0.0589, -0.1211, -0.0638,  0.0442, -0.2019,\n",
       "                       0.1163,  0.1370,  0.0567, -0.1589,  0.1922, -0.0091,  0.0929, -0.0359,\n",
       "                      -0.1351, -0.2176, -0.1807,  0.1302, -0.0173,  0.0786,  0.1011, -0.0014,\n",
       "                      -0.2233, -0.0178, -0.1185,  0.1277,  0.1593, -0.1924,  0.1806,  0.1740,\n",
       "                      -0.2088, -0.1846,  0.2160, -0.1061,  0.1546, -0.1392, -0.0477,  0.0173,\n",
       "                      -0.1239, -0.0193, -0.0905, -0.0231, -0.1681, -0.0042, -0.0064, -0.1060,\n",
       "                      -0.0703, -0.0491,  0.0160,  0.0250, -0.1473,  0.0573, -0.1075, -0.0754,\n",
       "                      -0.2214, -0.0728, -0.1240, -0.0939, -0.0735,  0.0388, -0.2233, -0.0945,\n",
       "                       0.1978,  0.1307,  0.0168, -0.2165,  0.2209, -0.0963, -0.1097,  0.1621,\n",
       "                      -0.0010, -0.0882,  0.0075,  0.0766,  0.0786,  0.1397,  0.1262, -0.0780,\n",
       "                      -0.1291,  0.0373, -0.1378, -0.0918,  0.0800, -0.1972,  0.1561, -0.0566,\n",
       "                      -0.1291,  0.0605,  0.2139,  0.2015,  0.1827, -0.1009, -0.1099,  0.0575]))])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(type(net[0].named_parameters()))#是个迭代器所以可以用for循环访问索引以及对应的值\n",
    "print(type(net[0].state_dict()))#是个有序字典不能用for循环进行访问\n",
    "net[0].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ffc14",
   "metadata": {},
   "source": [
    "**从嵌套块收集参数**\n",
    "    \n",
    "下段代码定义了一个生成块的函数，并将块嵌套成更大的块，其中的层是分层嵌套的，可以通过嵌套索引的方式访问参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90bd2d2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         net\u001b[38;5;241m.\u001b[39madd_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,block1())\u001b[38;5;66;03m#在字符串中使用i变量的值\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m net\n\u001b[1;32m---> 11\u001b[0m rgnet\u001b[38;5;241m=\u001b[39m\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mSequential(block2(),nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     12\u001b[0m rgnet(X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                         nn.Linear(8,4),nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net=nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())#在字符串中使用i变量的值\n",
    "    return net\n",
    "\n",
    "rgnet=nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f79d4885",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rgnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrgnet\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rgnet' is not defined"
     ]
    }
   ],
   "source": [
    "print(rgnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7207968c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rgnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrgnet\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rgnet' is not defined"
     ]
    }
   ],
   "source": [
    "rgnet[0][1][0].bias.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ee53a3",
   "metadata": {},
   "source": [
    "### 5.2.2 参数初始化\n",
    "    \n",
    "之前我们都是默认参数由系统随机初始化，当然我们也可以自定义初始化方法。默认情况下，pytorch会根据一个范围均匀地初始化权重与偏置矩阵，这个范围是根据输入与输出的维度决定。我们可以用nn.init模块实现多种预置初始化方法。\n",
    "    \n",
    "**内置初始化**\n",
    "\n",
    "下段代码将所有权重初始化为标准差为0.01的高斯随机变量，偏置设为0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7214fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mnormal_(m\u001b[38;5;241m.\u001b[39mweight,mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      5\u001b[0m         nn\u001b[38;5;241m.\u001b[39minit\u001b[38;5;241m.\u001b[39mzeros_(m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241m.\u001b[39mapply(init_constant)\u001b[38;5;66;03m#net间接的调用init_constant函数\u001b[39;00m\n\u001b[0;32m      8\u001b[0m net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m],net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    #m的类型是nn.Linear\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "        \n",
    "net.apply(init_constant)#net间接的调用init_constant函数\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009e532",
   "metadata": {},
   "source": [
    "下段代码将参数初始化为给定的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b80add2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1.]), tensor(0.))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "net[0].weight.data[0],net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9327e5",
   "metadata": {},
   "source": [
    "下段代码使用Xavier初始化方法初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e56e5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0101, 0.2327, 0.4561, 0.2999])\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42.]])\n"
     ]
    }
   ],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)#用均匀分布填充张量\n",
    "def init_42(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)#常数的值为42\n",
    "        \n",
    "net[0].apply(init_xavier)\n",
    "net[2].apply(init_42)\n",
    "print(net[0].weight.data[0])\n",
    "print(net[2].weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3066acc2",
   "metadata": {},
   "source": [
    "**自定义初始化**\n",
    "\n",
    "我们用任意的分布为权重参数初始化，深度学习框架是没法提供这样的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa26adb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m#如果参数的绝对值大于等于5，则参数保持不变；若绝对值小于5，则将参数置为零\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39mm\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241m.\u001b[39mapply(my_init)\n\u001b[0;32m     10\u001b[0m net[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mweight[:\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "def my_init(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        print('Init',*[(name,param.shape)\n",
    "                      for name,param in m.named_parameters()][0])\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        #如果参数的绝对值大于等于5，则参数保持不变；若绝对值小于5，则将参数置为零\n",
    "        m.weight.data*=m.weight.data.abs()>=5\n",
    "\n",
    "net.apply(my_init)\n",
    "net[0].weight[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b94bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "当然，我们可以直接设置参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13404a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.0000, 12.9929, 12.9995, 13.0015])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data[:]+=1\n",
    "net[0].weight.data[0,0]=42\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583ac24",
   "metadata": {},
   "source": [
    "### 5.2.3 参数绑定\n",
    "\n",
    "我们可以先定义一个层，然后使用它的参数来设置另一个层的参数，以此来实现参数共享。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1581f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True, True, True, True])\n",
      "tensor([True, True, True, True, True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "shared=nn.Linear(8,8)\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),\n",
    "                  shared,nn.ReLU(),\n",
    "                  shared,nn.ReLU(),\n",
    "                  nn.Linear(8,1))\n",
    "net(X)\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0])\n",
    "net[2].weight.data[0,0]=100\n",
    "#确保它们是同一个对象，而不是只有相同的值\n",
    "print(net[2].weight.data[0]==net[4].weight.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
