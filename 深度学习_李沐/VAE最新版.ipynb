{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d614e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "anomaly detection by vae for time series\n",
    "by smileyan (root@smileyan.cn)\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def reparameterize(mu, log_var):\n",
    "    \"\"\"重参数化，计算隐变量 z = μ + ε ⋅ σ\n",
    "    :param mu:  均值\n",
    "    :param log_var: 方差的 log 值\n",
    "    :return: 隐变量 z\n",
    "    \"\"\"\n",
    "    # log σ^2 -> σ\n",
    "    std = tf.exp(log_var * 0.5)\n",
    "    eps = tf.random.normal(std.shape)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "def log_normal_pdf(sample, mean, log_var, axis=1):\n",
    "    \"\"\" 求解正态分布(mean, var) 中的概率密度\n",
    "    :param sample: 待求样本\n",
    "    :param mean: 分布的均值\n",
    "    :param log_var: 分布的方差的对数值\n",
    "    :param axis: reduce_sum 的参数\n",
    "    :return: 概率密度的对数值\n",
    "    \"\"\"\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(-.5 * ((sample - mean) ** 2. * tf.exp(-log_var) + log_var + log2pi), axis=axis)\n",
    "\n",
    "\n",
    "class BasicVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_size=4, data_shape=120):\n",
    "        super(BasicVAE, self).__init__()\n",
    "        # input => h\n",
    "        self.fc1 = tf.keras.layers.Dense(100)\n",
    "        # h => μ and log σ^2\n",
    "        self.fc2 = tf.keras.layers.Dense(latent_size)\n",
    "        self.fc3 = tf.keras.layers.Dense(latent_size)\n",
    "\n",
    "        # sampled z => h\n",
    "        self.fc4 = tf.keras.layers.Dense(100)\n",
    "        # h => original data\n",
    "        self.fc5 = tf.keras.layers.Dense(data_shape)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"encode过程，返回 μ 和 log σ^2\n",
    "        :param x: 单窗口数据\n",
    "        :return:  μ 和 log σ^2\n",
    "        \"\"\"\n",
    "        h = tf.nn.relu(self.fc1(x))\n",
    "        # mu, log_variance\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "\n",
    "    def decode_logits(self, z):\n",
    "        h = tf.nn.relu(self.fc4(z))\n",
    "        return self.fc5(h)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return tf.nn.sigmoid(self.decode_logits(z))\n",
    "\n",
    "    def reconstruction_prob(self, x, sample_times=10):\n",
    "        \"\"\"计算本次数据的重构概率 E_{q_φ(z|x)}[log p_θ (x|z)]\n",
    "        :param x: 测试数据\n",
    "        :param sample_times: 采样时间\n",
    "        :return: 重构概率\n",
    "        \"\"\"\n",
    "        # tf.reshape(x, [-1, 120])\n",
    "        mean, log_var = self.encode(x)\n",
    "        samples_z = []\n",
    "        for i in range(sample_times):\n",
    "            z = reparameterize(mean, log_var)\n",
    "            samples_z.append(z)\n",
    "        reconstruction_prob = []\n",
    "        for z in samples_z:\n",
    "            x_logit = self.decode_logits(z)\n",
    "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "            # only take the last point's reconstruction probability\n",
    "            reconstruction_prob.append(cross_ent[0][-1])\n",
    "\n",
    "        return tf.reduce_mean(reconstruction_prob, axis=-1)\n",
    "\n",
    "    def compute_loss(self, x):\n",
    "        \"\"\"loss function, i.e., ELBO\n",
    "           :param x: one batch of validation set\n",
    "           :return: elbo\n",
    "       \"\"\"\n",
    "        mean, log_var = self.encode(x)\n",
    "        z = reparameterize(mean, log_var)\n",
    "        x_logit = self.decode_logits(z)\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "        log_p_x_z = -tf.reduce_sum(cross_ent)\n",
    "        log_p_z = log_normal_pdf(z, 0., 0.)\n",
    "        log_q_z_x = log_normal_pdf(z, mean, log_var)\n",
    "        return -tf.reduce_mean(log_p_x_z + log_p_z - log_q_z_x)\n",
    "\n",
    "    def fit(self, dataset_x, train_epochs=10, batch_size=100, optimizer=tf.keras.optimizers.Adam(1e-3), train_rate=0.7):\n",
    "        \"\"\"train vae model\n",
    "        :param dataset_x: window based data\n",
    "        :param batch_size: size of each batch default 10\n",
    "        :param train_rate: training data rate default 0.7\n",
    "        :param train_epochs: training times default 100\n",
    "        :param optimizer: one optimizer default Adam\n",
    "        \"\"\"\n",
    "        data_split = int(len(dataset_x) * train_rate)\n",
    "        train_data = tf.cast(dataset_x[:data_split], tf.float32)\n",
    "        test_data = tf.cast(dataset_x[data_split:], tf.float32)\n",
    "\n",
    "        # num_batches = data_split // batch_size\n",
    "        training_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(batch_size)\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(batch_size)\n",
    "\n",
    "        for epoch in range(train_epochs):\n",
    "            start_time = time.time()\n",
    "            for step, x in enumerate(training_dataset):\n",
    "                x = tf.reshape(x, [-1, 120])\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = self.compute_loss(x)\n",
    "                gradients = tape.gradient(loss, self.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "            train_end_time = time.time()\n",
    "            loss_mean = tf.keras.metrics.Mean()\n",
    "            for test_x in test_dataset:\n",
    "                test_x = tf.reshape(test_x, [-1, 120])\n",
    "                loss_mean(self.compute_loss(test_x))\n",
    "            elbo = -loss_mean.result()\n",
    "            test_end_time = time.time()\n",
    "            print('Epoch: {}/{}, test set ELBO: {:.4f}, train time elapse : {:.2f} s, test time elapse : {:.2f} s'\n",
    "                  .format(epoch+1, train_epochs, elbo, train_end_time - start_time, test_end_time - train_end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cef19368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of train_value_windows is 12179.\n",
      "The size of test_value_windows is 5151.\n",
      "Epoch: 1/2, test set ELBO: -38.2241, train time elapse : 202.21 s, test time elapse : 41.06 s\n",
      "Epoch: 2/2, test set ELBO: -37.8374, train time elapse : 262.07 s, test time elapse : 41.09 s\n",
      "The best threshold: 0.3977\n",
      "The best f1-score: 0.4375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#from vae.basic_vae import BasicVAE\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./sample_data.csv\")\n",
    "test_portion = 0.3\n",
    "test_n = int(df.shape[0] * test_portion)\n",
    "train_values, test_values = df['value'].values[:-test_n], df['value'].values[-test_n:]\n",
    "train_labels, test_labels = df['label'].values[:-test_n], df['label'].values[-test_n:]\n",
    "\n",
    "# window for train without anomalies\n",
    "train_value_windows = []\n",
    "for i in range(len(train_values)-119):\n",
    "    train_value_windows.append(train_values[i: i + 120])\n",
    "\n",
    "# test windows\n",
    "test_value_windows = []\n",
    "for i in range(test_n-119):\n",
    "    test_value_windows.append(test_values[i: i + 120])\n",
    "\n",
    "print('The size of train_value_windows is {}.'.format(len(train_value_windows)))\n",
    "print('The size of test_value_windows is {}.'.format(len(test_value_windows)))\n",
    "\n",
    "vae_model = BasicVAE()\n",
    "vae_model.fit(train_value_windows, batch_size=1, train_epochs=2)\n",
    "\n",
    "test_value_windows = tf.cast(test_value_windows, tf.float32)\n",
    "test_batches = tf.data.Dataset.from_tensor_slices(test_value_windows).batch(1)\n",
    "\n",
    "reconstructions = [vae_model.reconstruction_prob(window, 10) for window in test_batches]\n",
    "precisions, recalls, thresholds = precision_recall_curve(test_labels[-len(reconstructions):], reconstructions)\n",
    "\n",
    "# get best f1-score and best threshold\n",
    "f1_scores = (2 * precisions * recalls) / (precisions + recalls)\n",
    "best_f1_score = np.max(f1_scores[np.isfinite(f1_scores)])\n",
    "best_thresholds = thresholds[np.argmax(f1_scores[np.isfinite(f1_scores)])]\n",
    "\n",
    "print('The best threshold: {:.4f}'.format(best_thresholds))\n",
    "print('The best f1-score: {:.4f}'.format(best_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c306d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ec5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
