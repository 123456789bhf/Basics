{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7d4274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod, ABC\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Normal, kl_divergence\n",
    "from torch.nn.functional import softplus\n",
    "\n",
    "\n",
    "def tabular_encoder(input_size: int, latent_size: int):\n",
    "    \"\"\"\n",
    "    Simple encoder for tabular data.\n",
    "    If you want to feed image to a VAE make another encoder function with Conv2d instead of Linear layers.\n",
    "    :param input_size: number of input variables\n",
    "    :param latent_size: number of output variables i.e. the size of the latent space since it's the encoder of a VAE\n",
    "    :return: The untrained encoder model\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(input_size, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, latent_size * 2)  # times 2 because this is the concatenated vector of latent mean and variance\n",
    "    )\n",
    "\n",
    "\n",
    "def tabular_decoder(latent_size: int, output_size: int):\n",
    "    \"\"\"\n",
    "    Simple decoder for tabular data.\n",
    "    :param latent_size: size of input latent space\n",
    "    :param output_size: number of output parameters. Must have the same value of input_size\n",
    "    :return: the untrained decoder\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(latent_size, 200),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(200, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, output_size * 2)\n",
    "        # times 2 because this is the concatenated vector of reconstructed mean and variance\n",
    "    )\n",
    "\n",
    "\n",
    "class VAEAnomalyDetection(nn.Module, ABC):\n",
    "    def __init__(self, input_size: int, latent_size: int, L=10):\n",
    "        \"\"\"\n",
    "        :param input_size: Number of input features\n",
    "        :param latent_size: Size of the latent space\n",
    "        :param L: Number of samples in the latent space (See paper for more details)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.L = L\n",
    "        self.input_size = input_size\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = tabular_encoder(input_size, latent_size)\n",
    "        self.decoder = tabular_decoder(latent_size, input_size)\n",
    "        self.prior = Normal(0, 1)\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_encoder(self, input_size, latent_size):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_decoder(self, latent_size, output_size):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred_result = self.predict(x)\n",
    "        x = x.unsqueeze(0)  # unsqueeze to broadcast input across sample dimension (L)\n",
    "        log_lik = Normal(pred_result['recon_mu'], pred_result['recon_sigma']).log_prob(x).mean(\n",
    "            dim=0)  # average over sample dimension\n",
    "        log_lik = log_lik.mean(dim=0).sum()\n",
    "        kl = kl_divergence(pred_result['latent_dist'], self.prior).mean(dim=0).sum()\n",
    "        loss = kl - log_lik\n",
    "        return dict(loss=loss, kl=kl, recon_loss=log_lik, **pred_result)\n",
    "\n",
    "    def predict(self, x) -> dict:\n",
    "        \"\"\"\n",
    "        :param x: tensor of shape [batch_size, num_features]\n",
    "        :return: A dictionary containing prediction i.e.\n",
    "        - latent_dist = torch.distributions.Normal instance of latent space\n",
    "        - latent_mu = torch.Tensor mu (mean) parameter of latent Normal distribution\n",
    "        - latent_sigma = torch.Tensor sigma (std) parameter of latent Normal distribution\n",
    "        - recon_mu = torch.Tensor mu (mean) parameter of reconstructed Normal distribution\n",
    "        - recon_sigma = torch.Tensor sigma (std) parameter of reconstructed Normal distribution\n",
    "        - z = torch.Tensor sampled latent space from latent distribution\n",
    "        \"\"\"\n",
    "        batch_size = len(x)\n",
    "        latent_mu, latent_sigma = self.encoder(x).chunk(2, dim=1) #both with size [batch_size, latent_size]\n",
    "        latent_sigma = softplus(latent_sigma)\n",
    "        dist = Normal(latent_mu, latent_sigma)\n",
    "        z = dist.rsample([self.L])  # shape: [L, batch_size, latent_size]\n",
    "        z = z.view(self.L * batch_size, self.latent_size)\n",
    "        recon_mu, recon_sigma = self.decoder(z).chunk(2, dim=1)\n",
    "        recon_sigma = softplus(recon_sigma)\n",
    "        recon_mu = recon_mu.view(self.L, *x.shape)\n",
    "        recon_sigma = recon_sigma.view(self.L, *x.shape)\n",
    "        return dict(latent_dist=dist, latent_mu=latent_mu,\n",
    "                    latent_sigma=latent_sigma, recon_mu=recon_mu,\n",
    "                    recon_sigma=recon_sigma, z=z)\n",
    "\n",
    "    def is_anomaly(self, x, alpha=0.05):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x:\n",
    "        :param alpha: Anomaly threshold (see paper for more details)\n",
    "        :return: Return a vector of boolean with shape [x.shape[0]]\n",
    "                 which is true when an element is considered an anomaly\n",
    "        \"\"\"\n",
    "        p = self.reconstructed_probability(x)\n",
    "        return p < alpha\n",
    "\n",
    "    def reconstructed_probability(self, x):\n",
    "        with torch.no_grad():\n",
    "            pred = self.predict(x)\n",
    "        recon_dist = Normal(pred['recon_mu'], pred['recon_sigma'])\n",
    "        x = x.unsqueeze(0)\n",
    "        p = recon_dist.log_prob(x).exp().mean(dim=0).mean(dim=-1)  # vector of shape [batch_size]\n",
    "        return p\n",
    "\n",
    "    def generate(self, batch_size: int=1) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from prior distribution, feed into decoder and get in output recostructed samples\n",
    "        :param batch_size:\n",
    "        :return: Generated samples\n",
    "        \"\"\"\n",
    "        z = self.prior.sample((batch_size, self.latent_size))\n",
    "        recon_mu, recon_sigma = self.decoder(z).chunk(2, dim=1)\n",
    "        recon_sigma = softplus(recon_sigma)\n",
    "        return recon_mu + recon_sigma * torch.rand_like(recon_sigma)\n",
    "\n",
    "\n",
    "class VAEAnomalyTabular(VAEAnomalyDetection):\n",
    "\n",
    "    def make_encoder(self, input_size, latent_size):\n",
    "        \"\"\"\n",
    "        Simple encoder for tabular data.\n",
    "        If you want to feed image to a VAE make another encoder function with Conv2d instead of Linear layers.\n",
    "        :param input_size: number of input variables\n",
    "        :param latent_size: number of output variables i.e. the size of the latent space since it's the encoder of a VAE\n",
    "        :return: The untrained encoder model\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_size, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, latent_size * 2)\n",
    "            # times 2 because this is the concatenated vector of latent mean and variance\n",
    "        )\n",
    "\n",
    "    def make_decoder(self, latent_size, output_size):\n",
    "        \"\"\"\n",
    "        Simple decoder for tabular data.\n",
    "        :param latent_size: size of input latent space\n",
    "        :param output_size: number of output parameters. Must have the same value of input_size\n",
    "        :return: the untrained decoder\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(latent_size, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, output_size * 2)  # times 2 because this is the concatenated vector of reconstructed mean and variance\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881ef11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
