{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求整数的根号，保留整数部门\n",
    "def sqrt(n):\n",
    "    if n<=1:\n",
    "        return n\n",
    "    left=0\n",
    "    right=n \n",
    "    while left<=right:\n",
    "        mid=(left+right)//2\n",
    "        if mid*mid==n:\n",
    "            return mid\n",
    "        elif mid*mid<n:\n",
    "            left=mid+1\n",
    "        import argparse\n",
    "        import gym\n",
    "        import numpy as np\n",
    "        from itertools import count\n",
    "        from collections import namedtuple\n",
    "        \n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "        import torch.optim as optim\n",
    "        from torch.distributions import Categorical\n",
    "        \n",
    "        # Cart Pole\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "        parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                            help='discount factor (default: 0.99)')\n",
    "        parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                            help='random seed (default: 543)')\n",
    "        parser.add_argument('--render', action='store_true',\n",
    "                            help='render the environment')\n",
    "        parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                            help='interval between training status logs (default: 10)')\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        \n",
    "        env = gym.make('CartPole-v0')\n",
    "        env.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        \n",
    "        \n",
    "        SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "        \n",
    "        \n",
    "        class Policy(nn.Module):\n",
    "            \"\"\"\n",
    "            implements both actor and critic in one model\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                super(Policy, self).__init__()\n",
    "                self.affine1 = nn.Linear(4, 128)\n",
    "        \n",
    "                # actor's layer\n",
    "                self.action_head = nn.Linear(128, 2)\n",
    "        \n",
    "                # critic's layer\n",
    "                self.value_head = nn.Linear(128, 1)\n",
    "        \n",
    "                # action & reward buffer\n",
    "                self.saved_actions = []\n",
    "                self.rewards = []\n",
    "        \n",
    "            def forward(self, x):\n",
    "                \"\"\"\n",
    "                forward of both actor and critic\n",
    "                \"\"\"\n",
    "                x = F.relu(self.affine1(x))\n",
    "        \n",
    "                # actor: choses action to take from state s_t\n",
    "                # by returning probability of each action\n",
    "                action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "        \n",
    "                # critic: evaluates being in the state s_t\n",
    "                state_values = self.value_head(x)\n",
    "        \n",
    "                # return values for both actor and critic as a tupel of 2 values:\n",
    "                # 1. a list with the probability of each action over the action space\n",
    "                # 2. the value from state s_t\n",
    "                return action_prob, state_values\n",
    "        \n",
    "        \n",
    "        model = Policy()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "        \n",
    "        def select_action(state):\n",
    "            state = torch.from_numpy(state).float()\n",
    "            probs, state_value = model(state)\n",
    "        \n",
    "            # create a categorical distribution over the list of probabilities of actions\n",
    "            m = Categorical(probs)\n",
    "        \n",
    "            # and sample an action using the distribution\n",
    "            action = m.sample()\n",
    "        \n",
    "            # save to action buffer\n",
    "            model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "        \n",
    "            # the action to take (left or right)\n",
    "            return action.item()\n",
    "        \n",
    "        \n",
    "        def finish_episode():\n",
    "            \"\"\"\n",
    "            Training code. Calcultes actor and critic loss and performs backprop.\n",
    "            \"\"\"\n",
    "            R = 0\n",
    "            saved_actions = model.saved_actions\n",
    "            policy_losses = [] # list to save actor (policy) loss\n",
    "            value_losses = [] # list to save critic (value) loss\n",
    "            returns = [] # list to save the true values\n",
    "        \n",
    "            # calculate the true value using rewards returned from the environment\n",
    "            for r in model.rewards[::-1]:\n",
    "                # calculate the discounted value\n",
    "                R = r + args.gamma * R\n",
    "                returns.insert(0, R)\n",
    "        \n",
    "            returns = torch.tensor(returns)\n",
    "            returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "            for (log_prob, value), R in zip(saved_actions, returns):\n",
    "                advantage = R - value.item()\n",
    "        \n",
    "                # calculate actor (policy) loss\n",
    "                policy_losses.append(-log_prob * advantage)\n",
    "        \n",
    "                # calculate critic (value) loss using L1 smooth loss\n",
    "                value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "        \n",
    "            # reset gradients\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # sum up all the values of policy_losses and value_losses\n",
    "            loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "        \n",
    "            # perform backprop\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            # reset rewards and action buffer\n",
    "            del model.rewards[:]\n",
    "            del model.saved_actions[:]\n",
    "        \n",
    "        \n",
    "        def main():\n",
    "            running_reward = 10\n",
    "        \n",
    "            # run inifinitely many episodes\n",
    "            for i_episode in count(1):\n",
    "        \n",
    "                # reset environment and episode reward\n",
    "                state = env.reset()\n",
    "                ep_reward = 0\n",
    "        \n",
    "                # for each episode, only run 9999 steps so that we don't\n",
    "                # infinite loop while learning\n",
    "                for t in range(1, 10000):\n",
    "        \n",
    "                    # select action from policy\n",
    "                    action = select_action(state)\n",
    "        \n",
    "                    # take the action\n",
    "                    state, reward, done, _ = env.step(action)\n",
    "        \n",
    "                    if args.render:\n",
    "                        env.render()\n",
    "        \n",
    "                    model.rewards.append(reward)\n",
    "                    ep_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "        \n",
    "                # update cumulative reward\n",
    "                running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        \n",
    "                # perform backprop\n",
    "                finish_episode()\n",
    "        \n",
    "                # log results\n",
    "                if i_episode % args.log_interval == 0:\n",
    "                    print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                          i_episode, ep_reward, running_reward))\n",
    "        \n",
    "                # check if we have \"solved\" the cart pole problem\n",
    "                if running_reward > env.spec.reward_threshold:\n",
    "                    print(\"Solved! Running reward is now {} and \"\n",
    "                          \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        if __name__ == '__main__':\n",
    "            main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sqrt(n):\n",
    "    if n<=1:\n",
    "        return n \n",
    "    left=0\n",
    "    right=n\n",
    "    while left<=right:\n",
    "        mid=(left+right)//2\n",
    "        if mid*mid==n:\n",
    "            return mid\n",
    "        elif mid*mid<n:\n",
    "            left=mid+1\n",
    "        else:\n",
    "            right=mid-1\n",
    "    return right\n",
    "sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 4, 4, 5, 6]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 合并k个升序列表\n",
    "import heapq\n",
    "def merge_k_sortnums(lsits):\n",
    "    n=len(lsits)\n",
    "    heap=[]\n",
    "    res=[]\n",
    "    for i,nums in enumerate(lsits):\n",
    "        heapq.heappush(heap,(nums[0],i,0))\n",
    "    while heap:\n",
    "        num,index,start=heapq.heappop(heap)\n",
    "        res.append(num)\n",
    "        if start<len(lsits[index])-1:\n",
    "            heapq.heappush(heap,(lsits[index][start+1],index,start+1))\n",
    "    return res\n",
    "\n",
    "merge_k_sortnums([[1, 4, 5], [1, 3, 4], [2, 6]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 排序方法\n",
    "- 选择排序\n",
    "- 冒泡排序\n",
    "- 快速排序\n",
    "- 归并排序\n",
    "- 堆排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6, 7, 28]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择排序\n",
    "def choice_sort(nums):\n",
    "    n=len(nums)\n",
    "    for i in range(n-1,-1,-1):\n",
    "        temp=i\n",
    "        for j in range(i):\n",
    "            if nums[j]>nums[temp]:\n",
    "                temp=j \n",
    "        nums[temp],nums[i]=nums[i],nums[temp]\n",
    "    return nums\n",
    "nums=[3,1,28,7,6]\n",
    "choice_sort(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6, 7, 28]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 冒泡排序\n",
    "def sort(nums):\n",
    "\n",
    "    n=len(nums)\n",
    "\n",
    "    for i in range(n-1,-1,-1):\n",
    "        for j in range(i):\n",
    "            if nums[j]>nums[j+1]:\n",
    "                nums[j],nums[j+1]=nums[j+1],nums[j]\n",
    "    return nums\n",
    "nums=[3,1,28,7,6]\n",
    "sort(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6, 7, 28]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 快速排序\n",
    "def quick_sort(nums):\n",
    "    if len(nums)<=1:\n",
    "        return nums\n",
    "    privot=nums[0]\n",
    "    \n",
    "    left=[num for num in nums if num<privot]\n",
    "    middle=[num for num in nums if num==privot]\n",
    "    right=[num for num in nums if num>privot]\n",
    "    return quick_sort(left)+middle+quick_sort(right)\n",
    "nums=[3,1,28,7,6]\n",
    "quick_sort(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 6, 7, 28]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 归并排序\n",
    "def merge(left,rigght):\n",
    "    res=[]\n",
    "    while left and rigght:\n",
    "        if left[0]<rigght[0]:\n",
    "            res.append(left.pop(0))\n",
    "        else:\n",
    "            res.append(rigght.pop(0))\n",
    "    if left:\n",
    "        res=res+left\n",
    "    if rigght:\n",
    "        res=res+rigght\n",
    "    return res\n",
    "def split(nums):\n",
    "    if len(nums)<=1:\n",
    "        return nums\n",
    "    n=len(nums)\n",
    "    mid=n//2\n",
    "    left=nums[:mid]\n",
    "    right=nums[mid:]\n",
    "    return merge(split(left),split(right))\n",
    "nums=[3,1,28,7,6]\n",
    "split(nums)\n",
    "# nums1=[1,3,4]\n",
    "# nums2=[1,2]\n",
    "# merge(nums1,nums2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 华为笔试题2\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def min_transfer_routes(start, breakfast, end, routes):\n",
    "    # 构建站点到公交路线的映射\n",
    "    station_to_routes = defaultdict(set)\n",
    "    \n",
    "    for i, route in enumerate(routes):\n",
    "        for station in route:\n",
    "            station_to_routes[station].add(i)\n",
    "    \n",
    "    # 初始化BFS\n",
    "    queue = deque([(start, -1, False, 0)])  # (当前站点, 当前路线, 是否经过早餐站, 已换乘次数)\n",
    "    visited = set()  # 记录访问过的站点和路线组合\n",
    "    visited.add((start, -1, False))\n",
    "    \n",
    "    while queue:\n",
    "        station, route, passed_breakfast, count = queue.popleft()\n",
    "        \n",
    "        # 检查是否到达终点并经过了早餐店\n",
    "        if station == end and passed_breakfast:\n",
    "            return count\n",
    "        \n",
    "        # 遍历所有包含当前站点的公交路线\n",
    "        for next_route in station_to_routes[station]:\n",
    "            # 遍历公交路线中的每个站点\n",
    "            for next_station in routes[next_route]:\n",
    "                new_passed_breakfast = passed_breakfast or (next_station == breakfast)\n",
    "                new_state = (next_station, next_route, new_passed_breakfast)\n",
    "                \n",
    "                # 检查是否已访问过此状态\n",
    "                if new_state not in visited:\n",
    "                    visited.add(new_state)\n",
    "                    # 如果是同一路线，不增加换乘次数；否则换乘次数+1\n",
    "                    if next_route == route:\n",
    "                        queue.append((next_station, next_route, new_passed_breakfast, count))\n",
    "                    else:\n",
    "                        queue.append((next_station, next_route, new_passed_breakfast, count + 1))\n",
    "    \n",
    "    # 如果没有路径满足条件，返回-1\n",
    "    return -1\n",
    "\n",
    "# 示例测试\n",
    "# start = 1\n",
    "# breakfast = 9\n",
    "# end = 15\n",
    "# routes = [\n",
    "#     [1, 2, 3, 4, 5],      # 第0条公交路线\n",
    "#     [3, 6, 7, 8, 9],      # 第1条公交路线\n",
    "#     [10, 9, 8, 11, 12],   # 第2条公交路线\n",
    "#     [5, 13, 14, 15],      # 第3条公交路线\n",
    "#     [4, 12, 14]           # 第4条公交路线\n",
    "# ]\n",
    "start = 1\n",
    "breakfast = 4\n",
    "end = 5\n",
    "routes = [\n",
    "    [1, 2, 3],  # 第0条路线\n",
    "    [3, 4, 5]   # 第1条路线\n",
    "]\n",
    "\n",
    "min_transfer_routes(start, breakfast, end, routes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 5], [3, 4], [1, 4], [2], [3, 4], [1, 4, 5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wall = [[1,2,2,1],[3,1,2],[1,3,2],[2,4],[3,1,2],[1,3,1,1]]\n",
    "wall1=[[1],[1],[1]]\n",
    "#输出：2\n",
    "def help(wall):\n",
    "    m=len(wall)\n",
    "    n=sum(wall[0])\n",
    "    res=float('inf')\n",
    "    \n",
    "    wall_new=[]\n",
    "    for num in wall:\n",
    "        length=len(num)\n",
    "        for i in range(1,length):\n",
    "            num[i]=num[i-1]+num[i]\n",
    "        wall_new.append(num[:-1])\n",
    "    print(wall_new)\n",
    "    for i in range(1,n+1):\n",
    "        count=0\n",
    "        for j in range(m):\n",
    "            if i not in wall_new[j]:\n",
    "                count+=1\n",
    "        res=min(res,count)\n",
    "    return res\n",
    "help(wall)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累积墙边缘位置: [[1, 3, 5], [3, 4], [1, 4], [2], [3, 4], [1, 4, 5]]\n",
      "最小跨越数: 2\n"
     ]
    }
   ],
   "source": [
    "def help(wall):\n",
    "    m = len(wall)\n",
    "    n = sum(wall[0])\n",
    "    res = float('inf')\n",
    "    wall_new = []\n",
    "    \n",
    "    # 构建每行的累积和列表，以标记砖块边缘\n",
    "    for row in wall:\n",
    "        cumulative_row = []\n",
    "        total = 0\n",
    "        for brick in row[:-1]:  # 不包含最后一个砖块，避免到达墙的末端\n",
    "            total += brick\n",
    "            cumulative_row.append(total)\n",
    "        wall_new.append(cumulative_row)\n",
    "    \n",
    "    print(\"累积墙边缘位置:\", wall_new)\n",
    "    \n",
    "    # 对于每个可能的边缘位置，计算跨过的行数\n",
    "    for i in range(1, n):\n",
    "        count = 0\n",
    "        for row_edges in wall_new:\n",
    "            if i not in row_edges:\n",
    "                count += 1\n",
    "        res = min(res, count)\n",
    "    \n",
    "    return res\n",
    "\n",
    "# 测试示例\n",
    "wall = [[1,2,2,1], [3,1,2], [1,3,2], [2,4], [3,1,2], [1,3,1,1]]\n",
    "print(\"最小跨越数:\", help(wall))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
